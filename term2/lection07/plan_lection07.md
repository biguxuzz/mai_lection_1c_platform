# Лекция 7: RAG (Retrieval-Augmented Generation) - теория

**Продолжительность:** 90 минут  
**Целевая аудитория:** Студенты 3 курса, Факультет информационных технологий  
**Курс:** Разработка на платформе 1С с использованием ИИ

## Цель лекции
Познакомить студентов с концепцией RAG (Retrieval-Augmented Generation) и ее практическим применением в разработке систем на базе LLM, особенно в контексте платформы 1С.

## Необходимые предзнания
- Основы работы с LLM (лекция 2)
- Промпт-инженерия и работа с API (лекция 4)
- Docker и контейнеризация (лекции 5-6)

---

## План лекции (100 минут)

> **Примечание:** Время может варьироваться в зависимости от глубины обсуждения практических проблем и вопросов студентов.

### Блок 1: Введение в RAG (25 минут)

#### 1.1 Проблемы стандартных LLM (7 минут)

**Ситуация:** Представьте, что вы разрабатываете помощника для программистов 1С. Пользователь спрашивает: *"Как правильно организовать транзакцию при записи больших объемов данных в регистр сведений?"*

**Проблемы стандартного подхода:**

1. **Устаревшие данные модели**
   - LLM обучена на данных до определенной даты (cutoff date)
   - Новые версии платформы 1С (например, 8.3.25) и их особенности не известны модели
   - Изменения в лучших практиках и рекомендациях не отражены

2. **Нехватка специализированных данных**
   - LLM обучена на общедоступных данных интернета
   - Внутренняя документация компании, стандарты кодирования недоступны
   - Специфические кейсы и решения проблем отсутствуют

3. **Лимит контекста**
   - Даже современные модели имеют ограничение (4K, 8K, 32K, 128K токенов)
   - Невозможно загрузить всю документацию 1С в контекст
   - Нужно выбирать только релевантную информацию

4. **Галлюцинации**
   - Модель может "придумывать" факты, которых нет в реальности
   - Например, несуществующие методы или параметры API платформы 1С

**Вопрос для обсуждения:** Как вы думаете, можно ли решить эти проблемы без переобучения модели?

---

#### 1.2 Что такое RAG? (8 минут)

**RAG = Retrieval-Augmented Generation**

Разберем по частям:

- **Retrieval (Поиск)** - извлечение релевантной информации из базы знаний
- **Augmented (Дополнение)** - обогащение промпта найденной информацией
- **Generation (Генерация)** - создание ответа на основе дополненного контекста

**Ключевая идея:** Вместо того чтобы полагаться только на знания, "зашитые" в параметры модели, мы динамически извлекаем нужную информацию из внешнего источника и добавляем её в промпт.

**Аналогия:** Представьте экзамен:
- **Обычный LLM** = экзамен "по памяти" (closed book exam)
- **RAG** = экзамен с учебниками (open book exam)

Модель получает доступ к "учебникам" (базе знаний) и может найти точную информацию для ответа.

---

#### 1.3 Как работает RAG? (10 минут)

**Схема работы RAG-системы:**

```
┌─────────────────────────────────────────────────────────────────┐
│                    ЭТАП 1: ИНДЕКСАЦИЯ (offline)                 │
└─────────────────────────────────────────────────────────────────┘

Документы (PDF, DOCX, код 1С, и т.д.)
          ↓
[Извлечение текста и структуры]
          ↓
[Разбиение на чанки (chunks)]
          ↓
[Embedding Model - создание векторных представлений]
          ↓
[Векторная база данных (pgvector, Qdrant, ChromaDB)]


┌─────────────────────────────────────────────────────────────────┐
│                    ЭТАП 2: ЗАПРОС (online)                      │
└─────────────────────────────────────────────────────────────────┘

Вопрос пользователя: "Как работает ИзвлечениеТекста в 1С?"
          ↓
[Embedding Model - векторизация вопроса]
          ↓
[Поиск похожих векторов в базе данных]
          ↓
[Получение топ-K наиболее релевантных чанков]
          ↓
[Формирование промпта: системный промпт + контекст + вопрос]
          ↓
[LLM генерирует ответ на основе предоставленного контекста]
          ↓
Ответ пользователю
```

**Ключевые компоненты:**

1. **Embedding Model** - модель для преобразования текста в числовые векторы
   - Примеры: `text-embedding-ada-002` (OpenAI), `multilingual-e5-large` (локальная)
   - Векторы сохраняют семантический смысл текста
   - Похожие по смыслу тексты имеют похожие векторы

2. **Векторная база данных** - специализированное хранилище для векторов
   - Поддерживает быстрый поиск ближайших соседей (k-NN search)
   - Примеры: pgvector, Qdrant, Milvus, ChromaDB, Weaviate

3. **Chunking** - разбиение документов на управляемые фрагменты
   - Чанк должен быть достаточно большим для содержательности
   - Но достаточно маленьким, чтобы уместиться в контекст

---

#### 1.4 Семантический поиск vs Ключевые слова (5 минут)

**Пример для 1С:**

Допустим, пользователь спрашивает: *"Как извлечь содержимое из PDF файла?"*

**Поиск по ключевым словам:**
- Ищет точные совпадения: "PDF", "извлечь", "содержимое"
- НЕ найдет документацию о методе `ИзвлечениеТекста`
- Не понимает синонимы и схожие концепции

**Семантический поиск (векторный):**
- Понимает СМЫСЛ запроса: нужно получить текст из документа
- Найдет документацию о `ИзвлечениеТекста`, даже если там нет слова "извлечь"
- Найдет примеры с `ПолучитьТекстИзФайла`, `ЧтениеДокумента`
- Понимает связь между "PDF файл" и "документ", "файл", "внешний файл"

**Демонстрация векторов:**

```python
# Псевдокод для понимания
text1 = "Как извлечь текст из PDF?"
text2 = "ИзвлечениеТекста из документа"
text3 = "Рецепт борща"

vector1 = embedding_model.encode(text1)  # [0.2, 0.8, 0.1, ..., 0.5]
vector2 = embedding_model.encode(text2)  # [0.3, 0.7, 0.15, ..., 0.45]
vector3 = embedding_model.encode(text3)  # [0.9, 0.1, 0.8, ..., 0.2]

# Косинусное сходство (от 0 до 1, где 1 = идентичны)
similarity(vector1, vector2) = 0.89  # Очень похожи!
similarity(vector1, vector3) = 0.12  # Совсем разные
```

---

### Блок 2: Chunking - стратегии разбиения документов (20 минут)

#### 2.1 Почему нужен chunking? (5 минут)

**Проблема:** У нас есть документация 1С объемом 5000 страниц. Как загрузить её в RAG-систему?

**Невозможно:**
- Загрузить весь документ целиком в контекст LLM
- Создать один вектор для всего документа (потеряется детальность)

**Решение:** Разбить на небольшие, логически связанные фрагменты (чанки).

**Требования к чанкам:**
- Должны быть самодостаточными (содержать контекст)
- Оптимальный размер: 200-1000 токенов
- Должны сохранять семантическую целостность
- Не разрывать важные структуры (код, таблицы, списки)

---

#### 2.2 Основные стратегии chunking (10 минут)

**1. Fixed-size chunking (фиксированный размер)**

```
Документ -> [Чанк 1: 500 токенов] [Чанк 2: 500 токенов] [Чанк 3: 500 токенов]
```

**Плюсы:**
- Простота реализации
- Предсказуемый размер

**Минусы:**
- Может разорвать логические блоки
- Теряется структура документа

---

**2. Chunking с перекрытием (overlap)**

```
Документ -> [Чанк 1: токены 1-500]
                    [Чанк 2: токены 400-900]
                            [Чанк 3: токены 800-1300]
```

**Параметры:**
- `chunk_size` = 500 токенов
- `overlap` = 100 токенов

**Плюсы:**
- Контекст не теряется на границах чанков
- Важная информация попадает в несколько чанков

**Минусы:**
- Дублирование информации
- Увеличенный объем хранилища

---

**3. Semantic chunking (семантическое разбиение)**

Разбиение по смыслу:
- По абзацам
- По секциям документа
- По логическим блокам

```markdown
# Работа с файлами в 1С
[ЧАНК 1: вся эта секция]

## ИзвлечениеТекста
Метод позволяет...
[ЧАНК 2: описание метода]

### Пример кода
...
[ЧАНК 3: пример кода с описанием]
```

**Плюсы:**
- Сохраняет логическую структуру
- Чанки содержательны и самодостаточны

**Минусы:**
- Переменный размер чанков
- Требует понимания структуры документа

---

**4. Recursive chunking (рекурсивное разбиение)**

Используется в LangChain:

```python
# Пример из библиотеки LangChain
RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]  # Приоритет разделителей
)
```

Алгоритм:
1. Пытается разбить по параграфам (`\n\n`)
2. Если чанк слишком большой, разбивает по строкам (`\n`)
3. Если всё ещё большой, разбивает по словам (` `)
4. В крайнем случае - по символам

---

#### 2.3 Продвинутые техники (5 минут)

**1. Обогащение чанков метаданными**

```json
{
  "chunk_id": "doc123_chunk_5",
  "content": "Процедура ПровестиДокумент() выполняет...",
  "metadata": {
    "document": "Руководство разработчика 1С 8.3",
    "section": "Проведение документов",
    "page": 234,
    "chapter": "Глава 5: Документы",
    "document_type": "documentation"
  }
}
```

Это позволяет:
- Фильтровать поиск по метаданным
- Предоставлять источники в ответе
- Ранжировать результаты

---

**2. Обогащение с помощью LLM**

Для каждого чанка генерируем дополнительный контекст:

```
Промпт для LLM:
"Проанализируй этот фрагмент документации. 
Опиши кратко (2-3 предложения):
- О чём этот фрагмент?
- Какую проблему он решает?
- В каком контексте документа находится?"

Чанк: [текст чанка]
```

Сохраняем и оригинальный чанк, и описание. Это улучшает поиск!

---

**3. Описание изображений и таблиц**

Если в документе есть диаграммы, скриншоты, таблицы:

```
[Изображение: диаграмма последовательности]
↓
Vision Language Model (VLM)
↓
Текстовое описание: "Диаграмма показывает процесс проведения 
документа в 1С: пользователь нажимает кнопку 'Провести', 
система проверяет права, выполняет движения регистров, 
записывает документ..."
↓
Добавляется к чанку как текст
```

---

### Блок 3: Векторные базы данных (20 минут)

#### 3.1 Что такое вектор и embedding? (7 минут)

**Вектор** - это массив чисел, представляющий текст в многомерном пространстве.

**Пример (упрощённый 3D вектор):**

```
"собака" -> [0.8, 0.2, 0.1]
"щенок"  -> [0.75, 0.25, 0.15]
"кот"    -> [0.7, 0.3, 0.05]
"банк"   -> [0.1, 0.1, 0.9]
```

В реальности: 384, 768, 1536 или 3072 измерения!

**Визуализация (2D проекция):**

```
     Животные
        ^
        |
    собака • • щенок
        | • кот
        |
        |----------------> Финансы
        |              • банк
```

**Косинусное сходство** - мера похожести векторов:

```
cos(θ) = (A · B) / (||A|| × ||B||)

Где:
- A · B = скалярное произведение
- ||A|| = длина вектора A

Результат от -1 до 1 (обычно нормализуют до 0..1)
```

---

#### 3.2 Популярные векторные базы данных (8 минут)

**1. pgvector (PostgreSQL расширение)**

```sql
-- Создание таблицы
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding VECTOR(768),  -- 768-мерный вектор
    metadata JSONB
);

-- Создание индекса для быстрого поиска
CREATE INDEX ON document_chunks 
USING ivfflat (embedding vector_cosine_ops);

-- Поиск похожих векторов
SELECT 
    content, 
    metadata,
    1 - (embedding <=> query_vector) AS similarity
FROM document_chunks
ORDER BY embedding <=> query_vector
LIMIT 5;
```

**Плюсы:**
- Интегрируется с PostgreSQL (который вы уже знаете)
- Хранение векторов и обычных данных в одной БД
- Поддержка транзакций, ACID

**Минусы:**
- Медленнее специализированных решений при больших объемах
- Ограниченные возможности масштабирования

---

**2. Qdrant (специализированная векторная БД)**

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

client = QdrantClient("localhost", port=6333)

# Создание коллекции
client.create_collection(
    collection_name="documents_1c",
    vectors_config=VectorParams(
        size=768, 
        distance=Distance.COSINE
    )
)

# Добавление векторов
client.upsert(
    collection_name="documents_1c",
    points=[{
        "id": 1,
        "vector": embedding_vector,
        "payload": {
            "content": "Текст документа",
            "source": "docs.pdf",
            "page": 42
        }
    }]
)

# Поиск
results = client.search(
    collection_name="documents_1c",
    query_vector=query_embedding,
    limit=5,
    query_filter={  # Фильтрация по метаданным
        "must": [
            {"key": "source", "match": {"value": "docs.pdf"}}
        ]
    }
)
```

**Плюсы:**
- Очень быстрый поиск
- Поддержка фильтров
- Кластеризация и масштабирование
- REST API и клиенты для разных языков

---

**3. ChromaDB (embedding database)**

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("1c_knowledge")

# Добавление документов (векторизация автоматическая)
collection.add(
    documents=["Текст 1", "Текст 2", "Текст 3"],
    metadatas=[
        {"source": "doc1.pdf"},
        {"source": "doc2.pdf"},
        {"source": "doc3.pdf"}
    ],
    ids=["id1", "id2", "id3"]
)

# Поиск (векторизация запроса автоматическая)
results = collection.query(
    query_texts=["Как провести документ?"],
    n_results=5
)
```

**Плюсы:**
- Простота использования
- Встроенная векторизация
- Хорошо для прототипов и небольших проектов

---

#### 3.3 Пример структуры хранения (5 минут)

**Таблица в pgvector:**

```sql
CREATE TABLE rag_chunks (
    -- Идентификация
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id),
    chunk_index INTEGER,
    
    -- Контент
    content TEXT NOT NULL,
    content_length INTEGER,
    
    -- Вектор
    embedding VECTOR(768) NOT NULL,
    
    -- Метаданные
    metadata JSONB,
    
    -- Для фильтрации
    source_type VARCHAR(50),  -- 'documentation', 'code', 'forum'
    language VARCHAR(10),      -- 'ru', 'en'
    platform_version VARCHAR(20), -- '8.3.23', '8.3.24'
    
    -- Служебные поля
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Индексы
CREATE INDEX idx_embedding ON rag_chunks 
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 100);

CREATE INDEX idx_metadata ON rag_chunks 
    USING gin (metadata);

CREATE INDEX idx_source_type ON rag_chunks(source_type);
```

**Пример записи:**

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "document_id": "123e4567-e89b-12d3-a456-426614174000",
  "chunk_index": 5,
  "content": "Процедура ПровестиДокумент() Экспорт\n    // Проверка заполнения реквизитов\n    ПроверитьЗаполнение();\n    ...",
  "content_length": 450,
  "embedding": [0.023, -0.154, 0.891, ..., 0.234],  // 768 чисел
  "metadata": {
    "document_name": "Руководство разработчика",
    "chapter": "Глава 8: Проведение документов",
    "page": 156,
    "has_code": true,
    "code_language": "1c",
    "tags": ["документы", "проведение", "транзакции"]
  },
  "source_type": "documentation",
  "language": "ru",
  "platform_version": "8.3.24"
}
```

---

### Блок 4: Практический пример RAG для 1С (15 минут)

#### 4.1 Сценарий использования (5 минут)

**Задача:** Создать AI-ассистента для разработчиков 1С, который отвечает на вопросы по документации и внутренним стандартам компании.

**База знаний:**
1. Официальная документация 1С (PDF, ~3000 страниц)
2. Внутренние стандарты кодирования (Markdown, ~200 страниц)
3. База решённых проблем из корпоративной wiki (HTML, ~500 статей)
4. Примеры кода из GitHub репозиториев компании

---

#### 4.2 Архитектура решения (5 минут)

```
┌─────────────────────────────────────────────────────────────┐
│                    КОМПОНЕНТЫ СИСТЕМЫ                       │
└─────────────────────────────────────────────────────────────┘

[Документы] → [Docling Service] → [Извлечённый текст + структура]
                                            ↓
                                    [Chunking Service]
                                            ↓
                                    [Embedding Model]
                                     (e5-large-multilingual)
                                            ↓
                                    [PostgreSQL + pgvector]
                                    
                                    
[Вопрос пользователя] → [Embedding Model] → [Векторный поиск]
                                                    ↓
                                            [Top-5 чанков]
                                                    ↓
                    [Промпт = System + Context + Question]
                                                    ↓
                                            [LLM - Qwen]
                                                    ↓
                                            [Ответ с источниками]
```

**Стек технологий:**
- **Python FastAPI** - API сервис
- **PostgreSQL + pgvector** - хранение векторов
- **sentence-transformers** - создание embeddings
- **Ollama + Qwen2.5** - LLM для генерации ответов
- **Docker Compose** - оркестрация сервисов

---

#### 4.3 Пример кода RAG-системы (5 минут)

```python
from sentence_transformers import SentenceTransformer
import psycopg2
from psycopg2.extras import RealDictCursor

class RAGSystem:
    def __init__(self):
        self.embedding_model = SentenceTransformer(
            'intfloat/multilingual-e5-large'
        )
        self.db_conn = psycopg2.connect(
            dbname="rag_db",
            user="postgres",
            password="password",
            host="localhost"
        )
    
    def search(self, query: str, top_k: int = 5):
        """Поиск релевантных чанков"""
        
        # 1. Векторизация запроса
        query_embedding = self.embedding_model.encode(query)
        
        # 2. Поиск в БД
        with self.db_conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute("""
                SELECT 
                    content,
                    metadata,
                    1 - (embedding <=> %s::vector) AS similarity
                FROM rag_chunks
                WHERE source_type = '1c_documentation'
                ORDER BY embedding <=> %s::vector
                LIMIT %s
            """, (query_embedding.tolist(), query_embedding.tolist(), top_k))
            
            results = cur.fetchall()
        
        return results
    
    def generate_answer(self, query: str):
        """Генерация ответа с использованием RAG"""
        
        # 1. Найти релевантный контекст
        search_results = self.search(query, top_k=5)
        
        # 2. Сформировать промпт
        context = "\n\n---\n\n".join([
            f"Источник: {r['metadata'].get('document', 'unknown')}\n"
            f"Содержание: {r['content']}"
            for r in search_results
        ])
        
        prompt = f"""Ты - эксперт по платформе 1С:Предприятие. 
Ответь на вопрос пользователя, используя предоставленный контекст.

КОНТЕКСТ:
{context}

ВОПРОС: {query}

ОТВЕТ (используй только информацию из контекста):"""
        
        # 3. Отправить в LLM
        response = self.call_llm(prompt)
        
        # 4. Вернуть ответ с источниками
        return {
            "answer": response,
            "sources": [
                {
                    "document": r['metadata'].get('document'),
                    "page": r['metadata'].get('page'),
                    "similarity": r['similarity']
                }
                for r in search_results
            ]
        }
    
    def call_llm(self, prompt: str) -> str:
        """Вызов локальной LLM через Ollama"""
        import requests
        
        response = requests.post('http://localhost:11434/api/generate',
            json={
                "model": "qwen2.5:7b",
                "prompt": prompt,
                "stream": False
            }
        )
        
        return response.json()['response']


# Использование
rag = RAGSystem()

answer = rag.generate_answer(
    "Как правильно использовать транзакции при массовой загрузке данных в 1С?"
)

print(f"Ответ: {answer['answer']}\n")
print("Источники:")
for source in answer['sources']:
    print(f"  - {source['document']}, стр. {source['page']} "
          f"(релевантность: {source['similarity']:.2f})")
```

**Вывод программы:**

```
Ответ: Для массовой загрузки данных в 1С рекомендуется использовать 
следующий подход:

1. Отключить контроль заполнения: УстановитьОтключениеКонтроля()
2. Использовать режим записи без проведения
3. Группировать операции в транзакции по 100-1000 записей
4. Использовать РегистрСведений.СоздатьМенеджерЗаписи() вместо объектной модели

Пример кода:
НачатьТранзакцию();
Попытка
    Для Каждого Строка Из ДанныеДляЗагрузки Цикл
        Менеджер = РегистрыСведений.МойРегистр.СоздатьМенеджерЗаписи();
        // ... заполнение реквизитов ...
        Менеджер.Записать();
    КонецЦикла;
    ЗафиксироватьТранзакцию();
Исключение
    ОтменитьТранзакцию();
    ВызватьИсключение;
КонецПопытки;

Источники:
  - Руководство разработчика 1С 8.3, стр. 234 (релевантность: 0.89)
  - Оптимизация производительности, стр. 56 (релевантность: 0.82)
  - Внутренние стандарты: Загрузка данных, стр. 12 (релевантность: 0.78)
```

---

### Блок 5: Альтернативы и дополнения к RAG (10 минут)

#### 5.1 Fine-tuning (дообучение модели) (3 минуты)

**Что это:**
Переобучение LLM на специфичных данных вашей предметной области.

**Когда использовать:**
- Нужен специфичный стиль ответов
- Частые запросы на одну и ту же тему
- Модель должна "знать" информацию наизусть

**Плюсы:**
- Знания встроены в модель
- Не нужно хранилище векторов
- Быстрее инференс (нет дополнительного поиска)

**Минусы:**
- Дорого (требует GPU, время)
- Сложно обновлять знания
- Риск переобучения
- Нужен большой качественный датасет

**Сравнение с RAG:**

| Аспект | RAG | Fine-tuning |
|--------|-----|-------------|
| Обновление знаний | Легко (добавить документы) | Сложно (переобучить) |
| Стоимость | Низкая | Высокая |
| Прозрачность | Видны источники | Черный ящик |
| Точность | Высокая (из документов) | Зависит от качества обучения |

**Вывод:** Для постоянно обновляемой документации 1С лучше RAG!

---

#### 5.2 Кэширование (2 минуты)

**Идея:** Сохранять ответы на частые вопросы.

```python
import redis

cache = redis.Redis(host='localhost', port=6379)

def get_answer_with_cache(question: str):
    # Проверить кэш
    cached = cache.get(f"answer:{question}")
    if cached:
        return cached.decode('utf-8')
    
    # Если нет - использовать RAG
    answer = rag.generate_answer(question)
    
    # Сохранить в кэш на 1 час
    cache.setex(f"answer:{question}", 3600, answer)
    
    return answer
```

**Когда полезно:**
- Есть типовые вопросы
- Нужно снизить нагрузку на LLM
- Экономия токенов API

---

#### 5.3 Web Search (поиск в интернете) (2 минуты)

**Идея:** Для актуальных данных использовать поисковые системы.

**Примеры инструментов:**
- Tavily API
- Bing Search API
- Google Custom Search

**Когда использовать:**
- Вопросы о текущих событиях
- Информация, которой нет в вашей базе
- Проверка актуальности данных

**Комбинация:**
```python
def smart_answer(question: str):
    # 1. Проверить внутреннюю базу (RAG)
    internal_results = rag.search(question)
    
    # 2. Если релевантность низкая - искать в интернете
    if internal_results[0]['similarity'] < 0.7:
        web_results = web_search(question)
        return generate_from_web(question, web_results)
    
    return rag.generate_answer(question)
```

---

#### 5.4 Гибридные подходы (3 минуты)

**Лучшая практика: комбинировать методы!**

**1. RAG + Fine-tuning**
- Fine-tune модель на стиле ответов и терминологии 1С
- Использовать RAG для актуальной информации

**2. RAG + Re-ranking**
- Получить 50 кандидатов через векторный поиск
- Использовать специализированную модель (cross-encoder) для точного ранжирования
- Выбрать топ-5 для контекста

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

# Получить кандидатов
candidates = rag.search(query, top_k=50)

# Переранжировать
scores = reranker.predict([
    (query, c['content']) for c in candidates
])

# Выбрать лучшие
top_chunks = sorted(
    zip(candidates, scores), 
    key=lambda x: x[1], 
    reverse=True
)[:5]
```

**3. Multi-query RAG**
- Генерировать несколько вариаций вопроса
- Искать по каждой вариации
- Объединять результаты

```python
def multi_query_rag(original_query: str):
    # LLM генерирует вариации
    variations = llm.generate(f"""
    Перефразируй вопрос 3 разными способами:
    {original_query}
    """)
    
    all_results = []
    for query in [original_query] + variations:
        results = rag.search(query, top_k=3)
        all_results.extend(results)
    
    # Удалить дубликаты, ранжировать
    unique_results = deduplicate_and_rank(all_results)
    
    return rag.generate_answer_with_context(
        original_query, 
        unique_results[:5]
    )
```

---

### Блок 6: Метрики качества и оптимизация (20 минут)

> **Примечание для лектора:** Этот раздел требует особого внимания, так как на практике возникают различные проблемы при настройке RAG-систем. Раздел включает как теоретические метрики, так и практические решения реальных проблем.

#### 6.1 Как оценить качество RAG? (8 минут)

**1. Метрики поиска (Retrieval)**

**Precision@K** - точность первых K результатов

```
Precision@5 = (количество релевантных в топ-5) / 5
```

**Recall@K** - полнота

```
Recall@5 = (количество найденных релевантных) / (всего релевантных)
```

**MRR (Mean Reciprocal Rank)** - средний обратный ранг

```
MRR = среднее(1 / ранг_первого_релевантного_документа)
```

**Пример:**
```
Запрос: "Как провести документ в 1С?"

Результаты поиска:
1. [РЕЛЕВАНТНЫЙ] Документ о проведении
2. [НЕ РЕЛЕВАНТНЫЙ] Общая информация
3. [РЕЛЕВАНТНЫЙ] Пример кода проведения
4. [РЕЛЕВАНТНЫЙ] Транзакции при проведении
5. [НЕ РЕЛЕВАНТНЫЙ] История изменений

Precision@5 = 3/5 = 0.6
MRR = 1/1 = 1.0 (первый результат релевантен)
```

---

**2. Метрики генерации (Generation)**

**Faithfulness (верность источникам)**
- Проверка, что ответ основан на предоставленном контексте
- Нет галлюцинаций

**Answer Relevancy (релевантность ответа)**
- Ответ действительно отвечает на вопрос
- Нет лишней информации

**Context Relevancy (релевантность контекста)**
- Найденные чанки действительно полезны для ответа

**Инструменты для оценки:**
- **RAGAS** (библиотека для оценки RAG-систем) - автоматическая оценка метрик
- **Human evaluation** (ручная оценка) - наиболее надежный способ
- **A/B тестирование** - сравнение разных конфигураций

**Практический пример оценки:**

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_relevancy

# Тестовый набор данных
test_dataset = {
    "question": ["Как провести документ в 1С?"],
    "answer": ["Для проведения документа используется метод Провести()..."],
    "contexts": [[
        "Документ о проведении...",
        "Пример кода проведения..."
    ]],
    "ground_truth": ["Метод Провести() вызывает движение регистров..."]
}

# Оценка
results = evaluate(
    dataset=test_dataset,
    metrics=[faithfulness, answer_relevancy, context_relevancy]
)

print(f"Faithfulness: {results['faithfulness']:.3f}")
print(f"Answer Relevancy: {results['answer_relevancy']:.3f}")
print(f"Context Relevancy: {results['context_relevancy']:.3f}")
```

**Критерии качества ответа (для ручной оценки):**
1. **Точность** - факты верны
2. **Полнота** - ответ покрывает вопрос
3. **Релевантность** - информация относится к вопросу
4. **Полезность** - ответ помогает решить задачу
5. **Читаемость** - ответ понятен и структурирован

---

#### 6.2 Оптимизация RAG-системы (7 минут)

**1. Оптимизация размера чанков**

Эксперимент:
```python
chunk_sizes = [200, 500, 1000, 1500]

for size in chunk_sizes:
    # Переиндексировать с новым размером
    chunks = create_chunks(documents, chunk_size=size)
    index_chunks(chunks)
    
    # Оценить на тестовом наборе
    metrics = evaluate_on_testset(test_questions)
    print(f"Size: {size}, Precision: {metrics['precision']}")
```

Обычно оптимум: **500-800 токенов**.

---

**2. Подбор модели embedding**

```python
models = [
    'intfloat/multilingual-e5-large',
    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
    'cointegrated/rubert-tiny2'
]

for model_name in models:
    model = SentenceTransformer(model_name)
    
    # Переиндексировать
    reindex_with_model(model)
    
    # Оценить
    metrics = evaluate(model)
    print(f"{model_name}: MRR={metrics['mrr']:.3f}")
```

Для русского языка + 1С хорошо работает: **multilingual-e5-large**

---

**3. Гибридный поиск (Hybrid Search)**

Комбинация векторного и keyword поиска:

```sql
-- Векторный поиск
SELECT *, 
    1 - (embedding <=> query_vector) AS vector_score
FROM rag_chunks
ORDER BY embedding <=> query_vector
LIMIT 20;

-- BM25 (keyword search)
SELECT *,
    ts_rank(content_tsv, query_tsquery) AS keyword_score
FROM rag_chunks
WHERE content_tsv @@ query_tsquery
ORDER BY keyword_score DESC
LIMIT 20;

-- Объединение с весами
SELECT *,
    (0.7 * vector_score + 0.3 * keyword_score) AS final_score
FROM (
    -- union векторного и keyword поиска
)
ORDER BY final_score DESC
LIMIT 5;
```

---

**4. Query expansion (расширение запроса)**

```python
def expand_query(query: str) -> List[str]:
    """Добавить синонимы и связанные термины"""
    
    # Для 1С-специфичных терминов
    expansions = {
        "документ": ["документ", "объект", "ДокументОбъект"],
        "провести": ["провести", "проведение", "Провести()"],
        "регистр": ["регистр", "РегистрСведений", "РегистрНакопления"]
    }
    
    expanded = [query]
    for term, synonyms in expansions.items():
        if term in query.lower():
            for syn in synonyms:
                expanded.append(query.replace(term, syn))
    
    return expanded
```

**5. Реранкинг результатов (Re-ranking)**

После получения топ-K кандидатов из векторного поиска, используем cross-encoder для более точного ранжирования:

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')

def rerank_results(query: str, candidates: List[dict], top_k: int = 5):
    """Переранжировать результаты для повышения точности"""
    
    # Подготовить пары (запрос, документ)
    pairs = [(query, c['content']) for c in candidates]
    
    # Получить скоры
    scores = reranker.predict(pairs)
    
    # Отсортировать и вернуть топ-K
    ranked = sorted(
        zip(candidates, scores),
        key=lambda x: x[1],
        reverse=True
    )
    
    return [item[0] for item in ranked[:top_k]]
```

**6. Оптимизация числа релевантных чанков (top_k)**

Экспериментирование с количеством чанков в контексте:

```python
top_k_values = [3, 5, 7, 10]

for k in top_k_values:
    chunks = search(query, top_k=k)
    answer = generate_answer(query, chunks)
    
    # Оценить качество
    metrics = evaluate_answer(query, answer, chunks)
    print(f"Top-K={k}: Precision={metrics['precision']:.3f}")
```

**Рекомендации:**
- Начинать с top_k=5
- Если ответы неполные - увеличить до 7-10
- Если много лишней информации - уменьшить до 3-4

---

#### 6.3 Практические проблемы и решения (5 минут)

**Проблема 1: Совместимость Embedding API с различными платформами**

**Ситуация:** При использовании n8n workflow обнаружилось, что нода OpenAI Embedding API не работает корректно с LLM Studio.

**Возможные причины:**
- Несовместимость форматов запросов/ответов
- Проблемы с аутентификацией между сервисами
- Различия в версиях API

**Решения:**

1. **Использование альтернативных embedding моделей через Ollama**

```python
# Вместо OpenAI Embedding API
import ollama

def get_embedding(text: str, model: str = "nomic-embed-text"):
    """Получение embedding через Ollama"""
    response = ollama.embeddings(model=model, prompt=text)
    return response['embedding']
```

2. **Локальные embedding модели**

```python
from sentence_transformers import SentenceTransformer

# Загрузить модель один раз
model = SentenceTransformer('intfloat/multilingual-e5-large')

def get_embedding_local(text: str):
    """Локальная генерация embeddings"""
    return model.encode(text).tolist()
```

**Проблема 2: Снижение качества поиска при переходе на Ollama**

**Симптомы:**
- Найденные документы не релевантны запросу
- Низкие значения similarity scores
- Ответы не соответствуют контексту

**Диагностика:**

```python
def diagnose_search_quality(query: str, top_k: int = 5):
    """Диагностика качества поиска"""
    
    # 1. Получить результаты
    results = search(query, top_k=top_k)
    
    # 2. Проверить similarity scores
    similarities = [r['similarity'] for r in results]
    print(f"Similarity scores: {similarities}")
    print(f"Mean similarity: {sum(similarities)/len(similarities):.3f}")
    
    # 3. Проверить наличие ключевых слов
    query_words = set(query.lower().split())
    for i, result in enumerate(results):
        content_words = set(result['content'].lower().split())
        overlap = len(query_words & content_words) / len(query_words)
        print(f"Result {i+1}: word overlap={overlap:.3f}, "
              f"similarity={result['similarity']:.3f}")
    
    # 4. Рекомендации
    if max(similarities) < 0.6:
        print("⚠️  ПРОБЛЕМА: Низкая релевантность!")
        print("   Рекомендации:")
        print("   - Проверить embedding модель")
        print("   - Убедиться, что модель поддерживает русский язык")
        print("   - Проверить нормализацию текста")
```

**Возможные решения:**

1. **Выбор правильной embedding модели для Ollama**

```python
# Проверить доступные модели
# ollama list

# Попробовать разные модели
models_to_test = [
    "nomic-embed-text",      # Мультиязычная, но может быть слабее
    "snowflake-arctic-embed", # Альтернатива
]

# Или использовать локальные модели через sentence-transformers
best_models = [
    'intfloat/multilingual-e5-large',      # Лучше для русского
    'cointegrated/rubert-tiny2',            # Легкая, русская
    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
]
```

2. **Нормализация текста перед векторизацией**

```python
import re

def normalize_text(text: str) -> str:
    """Нормализация текста для лучшего поиска"""
    # Удалить лишние пробелы
    text = re.sub(r'\s+', ' ', text)
    # Привести к нижнему регистру
    text = text.lower()
    # Удалить специальные символы (опционально)
    # text = re.sub(r'[^\w\s]', '', text)
    return text.strip()

# Применять перед векторизацией
normalized_query = normalize_text(query)
embedding = get_embedding(normalized_query)
```

3. **Использование гибридного поиска**

Если векторный поиск дает плохие результаты, комбинировать с keyword search:

```python
def hybrid_search(query: str, top_k: int = 5):
    """Гибридный поиск: векторный + keyword"""
    
    # Векторный поиск (может быть слабым)
    vector_results = vector_search(query, top_k=top_k * 2)
    
    # Keyword поиск (BM25 или простой)
    keyword_results = keyword_search(query, top_k=top_k * 2)
    
    # Объединить с весами
    combined = {}
    for r in vector_results:
        combined[r['id']] = {
            'content': r['content'],
            'score': r['similarity'] * 0.6  # Вес векторного поиска
        }
    
    for r in keyword_results:
        if r['id'] in combined:
            combined[r['id']]['score'] += r['score'] * 0.4
        else:
            combined[r['id']] = {
                'content': r['content'],
                'score': r['score'] * 0.4
            }
    
    # Отсортировать и вернуть топ-K
    sorted_results = sorted(
        combined.items(),
        key=lambda x: x[1]['score'],
        reverse=True
    )
    
    return [{'content': v['content'], 'score': v['score']} 
            for _, v in sorted_results[:top_k]]
```

**Проблема 3: Интеграция RAG в n8n workflow**

**Рекомендации для n8n:**

1. **Использовать HTTP Request ноды** вместо специализированных AI нод (если они не работают)
2. **Создать отдельный сервис** для RAG (FastAPI/Flask) и вызывать его через HTTP
3. **Использовать Code ноды** для обработки данных перед отправкой в LLM

**Пример workflow:**
```
[Триггер] → [HTTP Request: Получить embeddings] → 
[Code: Обработать результаты] → 
[HTTP Request: Векторный поиск] → 
[Code: Формирование промпта] → 
[HTTP Request: LLM генерация] → 
[Ответ пользователю]
```

---

### Блок 7: Практические советы и заключение (10 минут)

#### 7.1 Best Practices для RAG (5 минут)

**1. Качество данных > Количество**
- Лучше 100 качественных, чистых документов, чем 1000 зашумленных
- Удаляйте устаревшую информацию
- Структурируйте документацию

**2. Метаданные - ваш друг**
- Храните версию платформы, дату, автора
- Используйте для фильтрации
- Помогает в дебаге

**3. Мониторинг и логирование**
```python
# Логировать каждый запрос
log_entry = {
    "timestamp": datetime.now(),
    "query": user_question,
    "retrieved_chunks": [c['id'] for c in chunks],
    "similarities": [c['similarity'] for c in chunks],
    "answer_length": len(answer),
    "generation_time": time_taken
}
```

**4. A/B тестирование**
- Экспериментировать с параметрами
- Собирать обратную связь пользователей
- Итеративно улучшать

**5. Fallback стратегии**
```python
def robust_rag(query: str):
    try:
        # Попытка RAG
        if has_good_results(query):
            return rag_answer(query)
        else:
            # Fallback на базовую модель
            return llm_answer_without_context(query)
    except Exception as e:
        log_error(e)
        return "Извините, произошла ошибка. Попробуйте переформулировать вопрос."
```

---

#### 7.2 Частые ошибки (3 минуты)

**1. Слишком маленькие чанки**
- Теряется контекст
- Ответы неполные

**2. Игнорирование метаданных**
- Смешивается информация из разных версий
- Устаревшие данные в ответах

**3. Отсутствие фильтрации результатов**
- Низкая релевантность попадает в контекст
- Путает модель

**4. Недостаточное тестирование**
- Не проверяют на edge cases
- Не собирают метрики

**5. Игнорирование структуры документов**
- Разбивают таблицы
- Теряют списки и иерархию

---

#### 7.3 Домашнее задание (практическое) (2 минуты)

**Задача:** Создать простую RAG-систему для документации 1С

**Требования:**
1. Использовать предоставленный PDF с документацией (100-200 страниц)
2. Разбить на чанки (экспериментировать с размером)
3. Создать векторную базу (pgvector или ChromaDB)
4. Реализовать поиск и генерацию ответов
5. Протестировать на 10 вопросах

**Дополнительно (по желанию):**
- Добавить метаданные (номера страниц, разделы)
- Реализовать гибридный поиск
- Сделать веб-интерфейс (Streamlit или Gradio)

**Критерии оценки:**
- Качество чанкинга (сохранение структуры)
- Релевантность результатов поиска
- Качество ответов LLM
- Наличие источников в ответах
- Обработка ошибок

**Срок:** 2 недели

---

## Резюме лекции (5 минут)

### Ключевые моменты:

1. **RAG решает фундаментальные проблемы LLM:**
   - Устаревшие данные
   - Отсутствие специализированных знаний
   - Ограничения контекста

2. **Принцип работы RAG:**
   ```
   Запрос → Векторизация → Поиск → Контекст → LLM → Ответ
   ```

3. **Chunking критичен для качества:**
   - Правильный размер (500-800 токенов)
   - Перекрытие для контекста
   - Сохранение структуры

4. **Векторные БД:**
   - pgvector - для интеграции с PostgreSQL
   - Qdrant - для высокой производительности
   - ChromaDB - для быстрого прототипирования

5. **RAG vs альтернативы:**
   - RAG - для обновляемых знаний
   - Fine-tuning - для встраивания знаний
   - Hybrid - для лучшего результата

6. **Метрики качества RAG:**
   - Retrieval метрики: Precision@K, Recall@K, MRR
   - Generation метрики: Faithfulness, Answer Relevancy, Context Relevancy
   - Использование RAGAS для автоматической оценки
   - Ручная оценка для критически важных систем

7. **Оптимизация RAG-системы:**
   - Подбор размера чанков (экспериментирование)
   - Выбор embedding модели (особенно для русского языка)
   - Гибридный поиск (векторный + keyword)
   - Реранкинг результатов
   - Оптимизация top_k параметра

8. **Практические проблемы и решения:**
   - Совместимость embedding API с различными платформами
   - Проблемы качества поиска при использовании Ollama
   - Важность нормализации текста и выбора модели
   - Использование гибридного поиска при низком качестве векторного поиска
   - Интеграция RAG в workflow системы (n8n и др.)

---

### Полезные ресурсы:

**Библиотеки:**
- LangChain (https://python.langchain.com/)
- LlamaIndex (https://www.llamaindex.ai/)
- Haystack (https://haystack.deepset.ai/)

**Инструменты:**
- Qdrant (https://qdrant.tech/)
- pgvector (https://github.com/pgvector/pgvector)
- Sentence Transformers (https://www.sbert.net/)

**Статьи и туториалы:**
- "RAG from Scratch" by LangChain
- "Building RAG Applications" by Pinecone
- "Advanced RAG Techniques" by LlamaIndex

---

### Вопросы для самопроверки:

1. Чем RAG отличается от простого промптинга?
2. Какие метрики используются для оценки качества RAG?
3. Почему векторный поиск лучше keyword search для семантического поиска?
4. Когда лучше использовать fine-tuning вместо RAG?
5. Как выбрать оптимальный размер чанка?
6. Что делать, если качество векторного поиска низкое (низкие similarity scores)?
7. Какие embedding модели лучше использовать для русского языка и документации 1С?
8. Как диагностировать проблемы качества поиска в RAG-системе?

---

## Следующая лекция:

**Лекция 8: RAG - практика и продвинутые техники**

**План лекции:**

1. **Практические решения проблем RAG**
   - Решение проблем с качеством поиска (Ollama, embedding модели)
   - Оптимизация workflow в n8n и других оркестраторах
   - Диагностика и отладка RAG-систем

2. **GraphRAG - семантический поиск с графами знаний**
   - Концепция GraphRAG
   - Использование Neo4j для хранения графа знаний
   - Комбинация векторного поиска и графов
   - Практический пример: GraphRAG для документации 1С

3. **Продвинутые техники RAG**
   - Self-RAG (автоматическая оценка и перегенерация)
   - Adaptive RAG (выбор между RAG и direct LLM)
   - Multi-document RAG (работа с несколькими источниками)

4. **Интеграция в реальные системы**
   - Развертывание RAG-системы в Docker
   - Интеграция с платформой 1С
   - Создание веб-интерфейса для RAG
   - Мониторинг и логирование

**Практическая часть:**
- Настройка GraphRAG с Neo4j
- Решение реальных проблем качества поиска
- Демонстрация полного workflow

**До встречи на следующей лекции! Вопросы?** 🎓 







